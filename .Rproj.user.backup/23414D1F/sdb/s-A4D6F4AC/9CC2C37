{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Thesis Background\"\nauthor: \"Anthony Kallhoff\"\ndate: \"July 28, 2017\"\noutput:\n  pdf_document: default\n  html_document:\n    toc: yes\n  toc: yes\n---\n\n##Background\n\nPrior to social media, conventional intelligence gathering consisted of processing traditional news outlets, reconnaissance of actual geography, or direct communication with personal contacts. In these techniques, intel personnel were tasked with deciphering any information manually. Reconnaissance data has begun to be analyzed with the aid of computer algorithms to quickly comb through large areas of land in search of anomalies. Likewise, the intel community is beginning to realize the potential value of the tremendous amount of data generated daily on social media sites like Instagram. Just like conventional intelligence techniques, however, they are learning that the process of procuring and managing the information can be just as challenging as the analysis. For social media, these challenges include stripping data from the site, storing that data in a logical architecture, and giving analyst reliable tools that provide reproducibility to their analysis.\n\n##Problem Statement\n\nINSCOM(U.S. Army **In**telligence and **S**ecurity **Com**mand) personnel wish to bridge the gap between traditional gatherings techniques and the new information afforded to us by social media. To this end, we will develop an R package that will interface with the Instagram API to pull data related to specific parameters. The package will also be able to generate reproducible reports of that data at the command of the user. This data will be stored in an R dataframe to allow an analyst access to whatever attributes they need for their study.   \n\nA further breakdown of the objectives reveals four areas of focus:\n\n1. Discovering what tools already exist\n2. Exact procedures to be used in extracting and storing data\n3. Advanced analytics for smarter searching\n4. Packing tools for ease of use and reproducibility\n\n###Discovering Tools\n\nUsing the open source language _R_ as the framework for the package gives us access to a wide array of packages and techniques already created to interface with Instagram's API and extract data. Additionally, our other desired functionalities might be found partially or entirely in other existing _R_ packages. Finding these packages could help to avoid common pitfalls while also freeing up time to pursue more unique capacities for our package. \n\n###Extracting Data\n\nWhether modifying an existing packages functions, or creating unique ones, we will need to ensure that all the functionality requested is present. This includes creating a clear definition of what information is worth pulling. Furthermore, once extracted, there needs to be a clear architecture in place to store the data and to run analysis on it. \n\n###Smarter searching\n\nIf time permits, there are some analytical techniques that can be incorporated into the search itself. Many times, the entire scope of an incident is hidden from an analyst's eyes. However, computers enable us to uncover words/locations/people that share correlations of use with the original search. By using the rate of correlation between words in discovered posts an algorithim could point out other words that have been used frequently with the searched words. This could point the research in new directions or display unkown and colloquial synonyms.\n\n###Packaging\n\nOnce created, the package will still be of no use if it does not get into the hands of analysts. This also entails making the package user friendly, intuitive, and concise.  As a further goal with the project, we are also seeking to create means to ensure that research done with the package is reproducible. \n\n##Final Project Scope\n\nFor OPER 682, the scope of the project is to create the basic working functions and the first working edition of the package, leaving out goal number three from the thesis objectives.  This first draft of the package would include interactions to pull simple searches from Instagram. This data would then be stored in some format that lends itself to analysis. And then finally, this procedure would occur in a reproducible environment, allowing other analysts to perform the same data pull and get the same data. \n\n#Data\n\nA large component of this research is in devising the means to scrape data from Instagram. Consequently, the initial  investigation into our data set will be only a rough relfection of what the final product aims to deliver. However, this exploration helps us to frame the basic requirements for the application we are creating. \n\n###Initial Data Scraping Attempts\n\nThe most convenient method to interact with Instagram would be to interface with the Instagram API and pull data using their built in procedures. Such features have been developed by Pablo Barbera in a package called *instaR*. However, as of 2016, Instagram requires approval for most endpoints of their API. This may be an option, but would require further discussion with INSCOM to figure out the specifics of the \"corporation\" details that Instagram requires.  Another avenue is to create a program that searches through Instagram posts manually and scrapes the data it sees.  This can be accomplished by incorporating PHP into our R package. A potential source for PHP examples is the *instagram-php-scraper* on GitHub.<sup>[1](#1)</sup>\n\n###Example Instagram Data\n\nWithout a working way to scrape data from Instagram yet, we can still look at what data would be pulled in the future.  An example dataset can be pulled from *BirdSong Analytics*.<sup>[2](#2)</sup>  \n\nWe load the *readr* package to allow us to read in csv files with **read_csv()** which saves a noticeable amount of time with our large files. The other packages are used later to display the data more elegantly.\n\n```{r message = FALSE, warning = FALSE}\n#to read in files faster\nlibrary(readr)\n\n#to make the tables look better\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(kableExtra)\noptions(knitr.table.format = \"latex\")\n\n#used to read in json files\nlibrary(jsonlite)\n\n#data visualization\nlibrary(ggplot2)\n\n#easy conversion of unixtime stamps\nlibrary(anytime) \n```\n\nThe sources for all the files are recorded. \n\n\n\n```{r eval = FALSE}\n## Without this false code, the urls extend far too far.\n\n\n#These will be the file names used later\nfiles <- c(\"Posts\", \"Comments\", \"Followers\", \"Following\")\n\n#these urls lead to a zip file containing only the file we want\npostsURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganalyti\ncs.com_20170126_crayola_posts.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Expires=1\n507118490&Signature=7iWlg5TsIIBcbCpN8SjFtoefUFY%3D\"\ncommentsURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganalyt\nics.com_20170126_crayola_comments.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Expire\ns=1507118513&Signature=UbagzQoyoGCibGaiXRxz3ozq3tM%3D\"\nfollowersURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganaly\ntics.com_20170126_crayola_followers.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Expi\nres=1507118490&Signature=K51tmkcNXcbFyltHSFrsAzYZKtI%3D\"\nfollowingURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganaly\ntics.com_20170126_crayola_following.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Exp\nires=1507118490&Signature=DCXCwG2TPVIyFByCnmfQ6BrQP4s%3D\"\n\n\n#we create a structure for our use later\nurls <- c(postsURL, commentsURL, followersURL, followingURL)\n\n\n#these are the file names of the csv files we need in the zip\npostsFN <- \"20170126_crayola_posts_0.csv\"\ncommentsFN <- \"20170126_crayola_comments_0.csv\"\nfollowersFN <- \"20170126_crayola_followers_0.csv\"\nfollowingFN <- \"20170126_crayola_following_0.csv\"\n\n#again, the structures used later\nFNs <- c(postsFN, commentsFN, followersFN, followingFN)\n\n```\n\n```{r echo = FALSE}\n#These will be the file names used later\nfiles <- c(\"Posts\", \"Comments\", \"Followers\", \"Following\")\n\n#these urls lead to a zip file containing only the file we want\npostsURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganalytics.com_20170126_crayola_posts.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Expires=1507118490&Signature=7iWlg5TsIIBcbCpN8SjFtoefUFY%3D\"\ncommentsURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganalytics.com_20170126_crayola_comments.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Expires=1507118513&Signature=UbagzQoyoGCibGaiXRxz3ozq3tM%3D\"\nfollowersURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganalytics.com_20170126_crayola_followers.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Expires=1507118490&Signature=K51tmkcNXcbFyltHSFrsAzYZKtI%3D\"\nfollowingURL <- \"http://birdsong.dtt.downloadsnew.s3.amazonaws.com/birdsonganalytics.com_20170126_crayola_following.zip?AWSAccessKeyId=AKIAINN7UV7N72CYJYNQ&Expires=1507118490&Signature=DCXCwG2TPVIyFByCnmfQ6BrQP4s%3D\"\n\n#we create a structure for our use later\nurls <- c(postsURL, commentsURL, followersURL, followingURL)\n\n\n#these are the file names of the csv files we need in the zip\npostsFN <- \"20170126_crayola_posts_0.csv\"\ncommentsFN <- \"20170126_crayola_comments_0.csv\"\nfollowersFN <- \"20170126_crayola_followers_0.csv\"\nfollowingFN <- \"20170126_crayola_following_0.csv\"\n\n#again, the structures used later\nFNs <- c(postsFN, commentsFN, followersFN, followingFN)\n\n```\n\nFor each of our four source files (posts, comments, followers, and following), we will download the data to temporary file, read in the data, and close the temp file. \n\n```{r eval = TRUE, warning = FALSE, message = FALSE}\n\n#loops over all 4 files\nfor( i in 1:4)\n{\n  \n  temp <- tempfile() #open a temporary file\n  \n  download.file(urls[i], temp, mode = \"wb\") #save the zip file from the url to temp file\n  \n  df <- read_csv(unz(temp, FNs[i])) #save the csv file in the zip to a data frame file\n  \n  assign(paste0(\"df\", files[i]), df) #rename the data frame file\n  \n  unlink(temp) # close the temp file\n}\n\n```\n\nNow we have imported the example Crayola data.  It comes in four breakdowns: by post, by comments, by followers, and by following. \n\n\nThe Instagram Posts data (Table 1) contains information on who the user is, the type of post (video or picture), meta-data on the post, and its reception (likes and comments).  Every post is given a unique media ID.\n\n```{r }\nkable(dfPosts[1:4,], caption = \"Instagram Posts\", booktabs = T) %>%\n  ##this makes the striped look, scales it down, and makes it not go to a new\n  ## page\n  kable_styling(latex_options = c(\"striped\", \"scale_down\", \"hold_position\")) %>%\n  ##these keep the lines from being too long\n  column_spec(3, width = \"10em\" ) %>% \n  column_spec(4, width = \"10em\" ) %>%\n  column_spec(5, width = \"10em\" )\n```\n\nThe Instagram Comments data (Table 2) lists a post with similar data as from Posts data, but it then lists the comments after that post, along with its unique comment ID and the user who posted it. \n\n```{r }\nkable(dfComments[1:4,], caption = \"Instagram Comments\", booktabs = T) %>%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\", \"hold_position\")) %>%\n  column_spec(12, width = \"10em\") %>%\n  column_spec(14, width = \"10em\") %>%\n  column_spec(4, width = \"10em\") %>%\n  column_spec(6, width = \"10em\")\n```\n\nThe Instagram Followers data (Table 3) contains all followers of the target user, in this case \"Crayola\". Along with general user information listed below, it is noted that users have an unique user ID.\n\n```{r }\nkable(dfFollowers[1:4,], caption = \"Instagram Followers\", booktabs =T) %>%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\", \"hold_position\")) %>%\n  column_spec(3, width = \"10em\") %>%\n  column_spec(10, width = \"10em\") %>%\n  column_spec(11, width = \"10em\")\n```\n\n\\newpage\n\n\nThe last data frame, Instagram Following data (Table 4), contains the information on users which the account is currently following. \n\n```{r }\nkable(dfFollowing[1:4,], caption = \"Instagram Posts\", booktabs = T) %>%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\", \"hold_position\")) %>%\n  column_spec(3, width = \"10em\") %>%\n  column_spec(10, width = \"10em\") %>%\n  column_spec(11, width = \"10em\") %>%\n  column_spec(13, width = \"10em\")\n```\n\n\n#Scraping\n\nThe previous set of data is found online and serves only to guide our data decisions. The true goal is to be able to extract data directly from Instagram with a unique search parameter. The package referenced before, Instagram PHP Scraper<sup>[1](#1)</sup>, ended up working for an initial starting point. \n\nThe following installation instructions were carried out on a Windows 10 OS. The first step is to install a working version of PHP on a machine. We accomplished this with the use of XAMPP<sup>[3](#3)</sup>, a development environment for PHP. After that, the package called for the use of composer<sup>[4](#4)</sup>, a dependency manager for PHP.  Composer asked for the PHP path, and we point it to the PHP executable in the XAMPP folder. At this point, we can download and extract the GitHub package to a location of our choosing. Finally, we must navigate to the GitHub package folder in our command prompt and run **composer install**.  This concludes the installation steps. \n\nNext, we must write the PHP script to run.  The final script used at this stage is displayed below. It includes arguments to have the tag and the number of posts defined dynamically in the command line. Additionally, it will automatically output the instagram data to a JSON file and the information on the speed of the scrape to a csv file.    \n\n\n```{r eval = FALSE}\n<?php\n \nrequire_once 'vendor/autoload.php';\n \nuse InstagramScraper\\Instagram;\n \nerror_reporting(E_ALL);\nini_set(\"display_errors\", 1);\n \n$instagram = Instagram::withCredentials('anthony.kallhoff', 'OPER682password1');\n$instagram->login();\n \n //the searched tag is entered after command\n$tag = $argv[1];\n\n//the desired number of results is entered after the command\n$count = $argv[2];\n\n//record how long search takes\n$startsearch = microtime(true);\n\n$medias = $instagram->getMediasByTag($tag, $count); //sets the number of results returned\n\n$endsearch = microtime(true);\n\n//reocrd how long encoding takes\n$startencode = microtime(true);\n\n//name the json file to capture the tag, count, and date time\n$myfile = (string)$tag . (string)$count . \"_\" . (string)date('d-m-y(H-i-sT)') . \".json\";\n\n//where we start caring\nob_start();\n\n//output the results of the scrape\necho json_encode($medias, JSON_UNESCAPED_SLASHES | JSON_PRETTY_PRINT);\n\n//flush the output into a variable\n$contents = ob_get_flush();\n//put the output into our data file\nfile_put_contents($myfile,$contents);\n\n$endencode = microtime(true);\n\n//calculate the time difference\n$timesearch = $endsearch - $startsearch;\n$timeencode = $endencode - $startencode;\n\n\nif(file_exists('instagramScrapeTimes.csv')){\n\t$file = fopen('instagramScrapeTimes.csv', 'a');//which file to write to\t\n}\nelse{\n\t$file = fopen('instagramScrapeTimes.csv', 'w');//which file to write to\n\n\tfputcsv($file, array('Tag', 'Count', 'Search Time', 'Encode Time')); //structure the column names\n}\n\n$data = array($tag, $count, $timesearch, $timeencode); //list the data\n\nfputcsv($file,$data); //write the data\n\nfclose($file);//close the file\n\n \n?>\n```\n\nIn this example, we will return the first 3000 results of medias with he hashtag 'kanye'. Currently, the search parameters are defined in the script. To execute the code, we again use command prompt, navigate to the desire folder and run **php scrapingScript.php > output.json**, where scrapingScript.php is the name of our PHP script, and output.json is the name of the output file we wish to create. \n\nThe last step to working with your data in R is to read in the .JSON file. We accomplish this by using a the *jsonlite* package.\n```{r}\n\nkanye_data <- fromJSON(\"kanye3000_29-07-17(01-21-55CEST).json\", flatten = TRUE) \n##flatten will ensure nested data frames are converted to \n##regular 2 dimensional tables\n```\n\nNow that we have our data, we can do a little investigation to see what it looks like. \n\nThe default function returns a significant amount of information. Our dataframe has 23 variables, many of which are similar to those found in our example data. \n\n```{r}\ncolnames(kanye_data)\n```\n\nSomething interesting to know would be what time frame 3000 posts is covering. We can look at a distribution of the time stamps for all our posts.  For this, we will use the *anytime* library to convert our data, since the times are stored as UNIX time stamps. \n\n```{r warning= FALSE, message = FALSE}\nkanye_data %>%\nggplot(aes(x = anytime(createdTime))) +\n  geom_histogram(bins = 100) +\n  xlab(\"Time Stamp\") +\n  ggtitle(\"3,000 Instagram Posts from July 12th-14th\") \n```\n\nWe can see from this graph that amount of posting peaks around 1300-1500 and bottoms out around 0300-0600.\n\nIt might also be interesting our rates of comments and likes for video and image posts\n\n```{r warning = FALSE}\nkanye_data %>%\nggplot(aes(x = likesCount, y = commentsCount, color = type)) +\n  geom_jitter() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Likes\") +\n  ylab(\"Comments\") + \n  ggtitle(\"Likes vs. Comments for Images and Videos\", \"No apparent correlation between type and response\") \n```\n\nWe don't see much of a correlation between the type of post and its response here, but we do observe the anticpated correlation between numbers of likes and comments, namely that they increase together. \n\nSince our interest more lie with the process of getting the data, rather than the data itself, it would be interesting to view some information on the rate of scraping. The data below was collected by iterating count size from 10 to around 1000.\n\n```{r warning = FALSE}\ntime_data <- read_csv(\"instagramScrapeTimes.csv\")\n\ntime_data %>%\n  ggplot(aes(x = Count, y = SearchTime)) + \n  geom_point() + \n  geom_smooth(method='lm')+\n   xlab(\"Count\") +\n  ylab(\"Search Time (s)\") + \n  ggtitle(\"Linear Relationship Between Search Time and Count\") \n```\n\nAs we can see, we only face a linear increase in time of the search for an increase size of search. \n\nThis work lays the foundation of real exploration in working with actual data. Continued work from this point will be to understand and annotate what exists already, streamline functionality, expand capabilities, and seek direction for INSCOM.\n\n#Analytics\n\nThe heart of this project lies not in the analytical work, but in the creation of a toolset for the analyst. In effect, there is not a mathematical principal that is applied at this time. Future investigations might see the addition of basic machine learning to assist the analyst with their search criteria, but that avenue will be explored after the creation of a reusable and accessible base program. Therefore, in this section we will discuss efforts to convert this static R markdown file that is heavily reliant on outside PHP processes, into a self-contained shiny app that responds to user inputs and can be easily accessed and operated by anyone who has R installed on their machine. \n\nThe first step toward this goal was to modify the discovered PHP script from Electric Archaeology<sup>[5](#5)</sup> to include customizable arguments for tag and count size. Then in the sake of reproducibility, we added code snippets to auto generate the output in a json file with an informational name and to time the processes.  All of the changes are reflected in the PHP script shown earlier. \n\nThe next major hurdle is to either find a way to incorporate PHP functionality into an R application, or to recreate the functionality of the PHP package in R.\n\n\n#Outside Links\n<sup>1<a name=\"1\"></a></sup>[https://github.com/postaddictme/instagram-php-scraper](https://github.com/postaddictme/instagram-php-scraper)\n\n<sup>2<a name=\"2\"></a></sup>[https://insights.birdsonganalytics.com/instagram/demo/overview](https://insights.birdsonganalytics.com/instagram/demo/overview)\n\n<sup>3<a name=\"3\"></a></sup>[XAMPP Homepage- https://www.apachefriends.org/index.html](https://www.apachefriends.org/index.html)\n\n<sup>4<a name=\"4\"></a></sup>[Composer Homepage- https://getcomposer.org/](https://getcomposer.org/)\n\n<sup>5<a name=\"5\"></a></sup>[Electric Archaeology PHP Script](https://electricarchaeology.ca/2016/12/06/scraping-instagram-with-r-with-php/)\n\n\n",
    "created" : 1516823550787.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1171757677",
    "id" : "9CC2C37",
    "lastKnownWriteTime" : 1516823565,
    "last_content_update" : 1516823565120,
    "path" : "~/Oper 682/Deliverable 4/2017_OPER682_kallhoffaj_d4.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 11,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}